{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from xvfbwrapper import Xvfb\n",
    "\n",
    "#vdisplay = Xvfb()\n",
    "#vdisplay.start()\n",
    "\n",
    "# for environ\n",
    "import os\n",
    "\n",
    "# only using device 0\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"LIBGL_ALWAYS_SOFTWARE\"]=\"1\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# importing tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# import scipy\n",
    "import scipy, csv\n",
    "import pandas as pd\n",
    "import tensorflow_probability as tfp\n",
    "import cvxpy as cp\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# to display environment data\n",
    "# see https://gist.github.com/thomelane/79e97630ba46c45985a946cae4805885\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# allowing GPU memory growth to allocate only what we need\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config, graph = tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#env = wrappers.Monitor(env, 'video')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions of state space\n",
    "S_DIM = 4\n",
    "\n",
    "# number of available actions\n",
    "ACTIONS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states\n",
    "states = tf.placeholder(tf.float64, shape = (None, S_DIM,))\n",
    "\n",
    "# taken actions\n",
    "actions = tf.placeholder(tf.int64, shape = (None,))\n",
    "\n",
    "# rewards obtained\n",
    "disc_rewards = tf.placeholder(tf.float64, shape = (None,))\n",
    "\n",
    "# costs obtained\n",
    "disc_costs = tf.placeholder(tf.float64, shape = (None,))\n",
    "\n",
    "def fc_layer(x, n, activation = tf.nn.sigmoid):\n",
    "    \"\"\" Fully connected layer for input x and output dim n \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x, n, activation_fn=activation,\n",
    "    weights_initializer=tf.initializers.lecun_normal(), weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(), biases_regularizer=None, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers\n",
    "with tf.name_scope('layers'):\n",
    "    z = states # state is an input\n",
    "    #z = fc_layer(z, 5)\n",
    "    z = fc_layer(z, 10)\n",
    "    #z = fc_layer(z, 10)\n",
    "    z = fc_layer(z, ACTIONS, activation = None)\n",
    "    output = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax to make probability distribution\n",
    "logits = tf.nn.softmax(output)\n",
    "\n",
    "# predicted labels\n",
    "labels = tf.argmax(logits, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to optimize the sum of rewards:\n",
    "$$\n",
    "J(\\theta)=\\mathbb{E}_{\\pi(\\theta)}\\sum\\limits_{t=0}^\\infty r_t\n",
    "$$\n",
    "\n",
    "Take the gradient and use log-likelihood trick:\n",
    "$$\n",
    "\\nabla J(\\theta)=\\mathbb{E}_{\\pi}\\sum\\limits_{t=0}^\\infty r_t\\nabla_\\theta \\log \\pi(a_t|s_t)=\\nabla_\\theta\\mathbb{E}_{\\pi}\\sum\\limits_{t=0}^\\infty r_t\\log \\pi(a_t|s_t)\n",
    "$$\n",
    "\n",
    "Therefore the task is equivalent to minimizing a loss of\n",
    "$$\n",
    "\\mathbb{E}_{\\pi}\\sum\\limits_{t=0}^\\infty r_t\\log \\pi(a_t|s_t)\n",
    "$$\n",
    "\n",
    "Which is estimated stochastically using episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe algo 1: <a href=\"https://arxiv.org/pdf/1705.10528.pdf\">Constrained Policy Optimization</a>\n",
    "\n",
    "Estimating $g=\\nabla_\\theta J_r(\\pi)$ using policy gradients\n",
    "\n",
    "Estimating $b=\\nabla_\\theta J_C(\\pi)$ using policy gradients\n",
    "\n",
    "Estimating $H=\\mathbb{E}\\frac{\\partial^2}{\\partial \\theta^2}D_{KL}(\\theta'|\\theta)$\n",
    "\n",
    "Constants: $\\delta$ step size for trust region, $c=J_C-d$, where $d$ is the maximal per-episode value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate for policy gradients\n",
    "gamma_discount = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(rewards):\n",
    "    \"\"\" Discount and do cumulative sum \"\"\"\n",
    "    sum_so_far = 0.0\n",
    "    rewards_so_far = []\n",
    "    for r in rewards[::-1]:\n",
    "        sum_so_far = sum_so_far * gamma_discount + r\n",
    "        rewards_so_far.append(sum_so_far)\n",
    "    return rewards_so_far[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(discount([1,2,3]), [3 * gamma_discount ** 2 + 2 * gamma_discount + 1, 3 * gamma_discount + 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dz_dw_flatten(z, params):\n",
    "    \"\"\" Calculate dz/dparams and flatten the result \"\"\"\n",
    "    return tf.concat([tf.reshape(x, shape = (-1,)) for x in tf.gradients(z, params)], axis = 0)\n",
    "\n",
    "def iterate_flatten(tensor):\n",
    "    \"\"\" Iterate over flattened items of a tensor \"\"\"\n",
    "    if type(tensor) == list:\n",
    "        for t in tensor:\n",
    "            for v in iterate_flatten(t):\n",
    "                yield v\n",
    "    elif len(tensor.shape) == 0:\n",
    "        yield tensor\n",
    "    else:\n",
    "        for idx in range(tensor.shape[0]):\n",
    "            for v in iterate_flatten(tensor[idx]):\n",
    "                yield v\n",
    "                \n",
    "def tf_hessian(var, params):\n",
    "    # gradients of the loss w.r.t. params\n",
    "    grads = tf.gradients(var, params)\n",
    "    grad_components = list(iterate_flatten(grads))\n",
    "    hessian = [dz_dw_flatten(t, params) for t in tqdm(grad_components)]\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoded actions\n",
    "a_one_hot = tf.one_hot(actions, ACTIONS)\n",
    "\n",
    "# taken logits\n",
    "#logits_taken = tf.gather(logits, actions, axis = 1)\n",
    "logits_taken = tf.boolean_mask(logits, a_one_hot)\n",
    "\n",
    "# logarithm\n",
    "log_logits = tf.log(logits_taken)\n",
    "\n",
    "# calculated loss\n",
    "loss_r = -tf.reduce_sum(tf.multiply(log_logits, disc_rewards))\n",
    "loss_c = -tf.reduce_sum(tf.multiply(log_logits, disc_costs))\n",
    "\n",
    "# KL(pi(a)||pi(a) fixed)\n",
    "kl_div_var_fixed = tf.reduce_mean(tfp.distributions.kl_divergence(\n",
    "    tf.distributions.Categorical(probs = logits),\n",
    "    tf.distributions.Categorical(probs = tf.stop_gradient(logits))))\n",
    "    #tf.distributions.Categorical(probs = logits))) #!!! not using this because want variable first parameter and fixed second\n",
    "\n",
    "# policy gradient for reward\n",
    "g = list(iterate_flatten(tf.gradients(-loss_r, params)))\n",
    "\n",
    "# policy gradient for constraint\n",
    "b = list(iterate_flatten(tf.gradients(-loss_c, params)))\n",
    "\n",
    "# hessian of KL divergence (parameter H)\n",
    "H = tf_hessian(kl_div_var_fixed, params)\n",
    "\n",
    "# flattened parameters\n",
    "theta = list(iterate_flatten(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('loss_r', loss_r)\n",
    "tf.summary.scalar('loss_c', loss_c)\n",
    "tf.summary.scalar('g_norm', tf.norm(g))\n",
    "tf.summary.scalar('b_norm', tf.norm(b))\n",
    "tf.summary.scalar('mean_act', tf.reduce_mean(actions))\n",
    "tf.summary.scalar('episode_length', tf.reduce_sum(tf.shape(disc_costs)))\n",
    "tf.summary.scalar('reward', disc_rewards[0])\n",
    "tf.summary.scalar('cost', disc_rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tf.summary.merge_all()\n",
    "summary_i = 0\n",
    "s_writer = tf.summary.FileWriter('./tensorboard/' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(observation):\n",
    "    \"\"\" Sample an action from the policy \"\"\"\n",
    "    \n",
    "    p = sess.run(logits, feed_dict = {states: [observation]})[0]\n",
    "    return np.random.choice(range(2), p = p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(obs):\n",
    "    \"\"\" Calculate scalar cost of one observation \"\"\"\n",
    "    assert isinstance(obs, np.ndarray) and obs.shape == (4,), \"Input must be an np-array [x xdot phi phidot]\"\n",
    "    \n",
    "    # parsing input\n",
    "    x, x_dot, phi, phi_dot = obs\n",
    "    \n",
    "    #X_MAX = 1.0\n",
    "    #X_DOT_MAX = 0.5\n",
    "    #PHI_MAX = 0.1\n",
    "    #PHI_DOT_MAX = 0.5\n",
    "    \n",
    "    if x < 0 or phi < 0:\n",
    "        return 1\n",
    "    \n",
    "    #if np.any(np.abs([x, x_dot, phi, phi_dot]) > [X_MAX, X_DOT_MAX, PHI_MAX, PHI_DOT_MAX]):\n",
    "    #    return 1\n",
    "    \n",
    "    # in all other cases no cost\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rollout():\n",
    "    \"\"\" Obtain rollout using policy \"\"\"\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    sarc = []\n",
    "    while not done:\n",
    "        act = sample_action(observation)\n",
    "        observation_, reward, done, info = env.step(act) # take a random action\n",
    "        cost_ = cost(observation_)\n",
    "        global observations\n",
    "        observations += [observation_]\n",
    "        sarc.append((observation, act, reward, cost_))\n",
    "        observation = observation_\n",
    "    env.close()\n",
    "    return sarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OwnGradientDescent():\n",
    "    def __init__(self, gamma = 0.5, theta = 0.9):\n",
    "        # gamma (learning rate)\n",
    "        self.gamma = tf.Variable(gamma, dtype = tf.float32)\n",
    "        self.theta = theta\n",
    "        \n",
    "    def minimize(self, loss, params):\n",
    "        \"\"\" Minimize some loss \"\"\"\n",
    "        def decrement_weights(W, gamma, grads):\n",
    "            \"\"\" w = w - how_much \"\"\"\n",
    "            ops = [w.assign(tf.subtract(w, tf.multiply(gamma, grad))) for w, grad in zip(W, grads)]\n",
    "            return tf.group(ops)\n",
    "        \n",
    "        # gradients of the loss w.r.t. params\n",
    "        grads = tf.gradients(loss, params)\n",
    "        \n",
    "        # perform gradient descent step\n",
    "        train_op = decrement_weights(params, self.gamma, grads)\n",
    "        \n",
    "        # updating gamma\n",
    "        upd_op = self.gamma.assign(tf.multiply(self.gamma, self.theta))\n",
    "        \n",
    "        return tf.group(train_op, upd_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one learning iteration\n",
    "#step = tf.train.AdamOptimizer(0.1).minimize(loss_r)\n",
    "step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    # obtaining rollout data\n",
    "    S, A, R, C = list(zip(*get_rollout()))\n",
    "    sess.run(step, feed_dict = {states: S, actions: A, disc_rewards: discount(R), disc_costs: discount(C)})\n",
    "    return np.sum(R), np.sum(C)\n",
    "\n",
    "def get_params():\n",
    "    \"\"\" Current parameters \"\"\"\n",
    "    return np.array(list(iterate_flatten(sess.run(params))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(300)):\n",
    "    r, c = train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Rs, label = 'Reward')\n",
    "plt.plot(Cs, label = 'Cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_rollout())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: reimplement w/o inner optimization, otherwise too slow.\n",
    "\n",
    "## TODO: see how norm /delta evolves!\n",
    "## TODO: implement backtracking line search\n",
    "\n",
    "def solve_CPO(H_, b_, g_, L, J_R, J_C, delta = 0.1, d = 10, H_lambda = 0.01):\n",
    "    # CPO hyperparameters\n",
    "\n",
    "    # D_KL maximal distance\n",
    "    #delta\n",
    "\n",
    "    # maximal constraint return\n",
    "    #d\n",
    "\n",
    "    # matrix conditioner\n",
    "    #H_lambda\n",
    "    \n",
    "    # calculating H, g, b, J from obtained data\n",
    "    J_c_ = J_C\n",
    "    \n",
    "    # constraint dissatisfaction\n",
    "    c = J_c_ - d\n",
    "\n",
    "    # current parameters\n",
    "    theta_k = list(iterate_flatten(sess.run(params)))\n",
    "    \n",
    "    # https://www.cvxpy.org/\n",
    "\n",
    "    # Construct the problem.\n",
    "    theta = cp.Variable(len(theta_k))\n",
    "    objective = cp.Maximize(g_.T @ (theta - theta_k))\n",
    "    constraints = [c + b_.T @ (theta - theta_k) <= 0, cp.quad_form(theta - theta_k, H_) <= 2 * delta]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "\n",
    "    # The optimal objective value is returned by `prob.solve()`.\n",
    "    fallback = False\n",
    "    try:\n",
    "        result = prob.solve(gp=False)\n",
    "    except:\n",
    "        fallback = True\n",
    "    # The optimal value for x is stored in `x.value`.\n",
    "    #print(theta.value)\n",
    "    # The optimal Lagrange multiplier for a constraint is stored in\n",
    "    # `constraint.dual_value`.\n",
    "    #print(constraints[0].dual_value, constraints[1].dual_value)\n",
    "    \n",
    "    if fallback or theta.value is None:\n",
    "        print(J_c_, d)\n",
    "        print('Optimization failed: current policy nonsafe. Falling back to a safe solution...')\n",
    "        \n",
    "        # θ∗ = θk −sqrt(2δ/b^T H−1b)H^-1b. (eq 14)\n",
    "        Hinv = np.linalg.inv(H_)\n",
    "        theta_safe = theta_k - 0.01 * ((2 * delta / (b_.T @ Hinv @ b_)) ** 0.5 * Hinv @ b_).reshape(-1)\n",
    "        load_params(params, theta_safe)\n",
    "        return\n",
    "    \n",
    "    load_params(params, theta.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(params, vector):\n",
    "    \"\"\" Params: list of tensors, vector: array with new values (shape as flattened params) \"\"\"\n",
    "    # index to start\n",
    "    idx = 0\n",
    "    \n",
    "    # array with assign operators\n",
    "    assigns = []\n",
    "    \n",
    "    # loop over parameters\n",
    "    for p in params:\n",
    "        # length of current parameter (flattened)\n",
    "        curr_len = np.prod(p.shape)\n",
    "        \n",
    "        # new values for p\n",
    "        p_new_value = vector[idx:idx+curr_len]\n",
    "        \n",
    "        # reshaping the values\n",
    "        assigns.append(tf.assign(p, p_new_value.reshape(p.shape)))\n",
    "        \n",
    "        # incrementing start ID\n",
    "        idx += curr_len\n",
    "        \n",
    "    # all assigns in one operation\n",
    "    assigns = tf.group(assigns)\n",
    "    \n",
    "    # running the operation\n",
    "    sess.run(assigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gLRC():\n",
    "    S, A, R, C = zip(*get_rollout())\n",
    "    # calculating H, g, b, J from obtained data\n",
    "    J_c_ = discount(C)[0]\n",
    "    g_, summary_res = sess.run([g, summary], feed_dict = {states: S, actions: A, disc_rewards: discount(R), disc_costs: discount(C)})\n",
    "    global summary_i, s_writer\n",
    "    s_writer.add_summary(summary_res, summary_i)\n",
    "    summary_i += 1\n",
    "    return np.array([g_]).T, np.array(len(R)), np.array(discount(R)[0]), np.array(discount(C)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HbgLRC():\n",
    "    S, A, R, C = zip(*get_rollout())\n",
    "    # calculating H, g, b, J from obtained data\n",
    "    J_c_ = discount(C)[0]\n",
    "    global summary_i, s_writer\n",
    "    H_, b_, g_, summary_res = sess.run([H, b, g, summary], feed_dict = {states: S, actions: A, disc_rewards: discount(R), disc_costs: discount(C)})\n",
    "    s_writer.add_summary(summary_res, summary_i)\n",
    "    summary_i += 1\n",
    "    return np.array(H_), np.array([b_]).T, np.array([g_]).T, np.array(len(R)), np.array(discount(R)[0]), np.array(discount(C)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_HbgLRC(repetitions = 10):\n",
    "    values = zip(*[get_HbgLRC() for _ in range(repetitions)])\n",
    "    return [np.mean(x, axis = 0) for x in values]\n",
    "def average_gLRC(repetitions = 10):\n",
    "    values = zip(*[get_gLRC() for _ in range(repetitions)])\n",
    "    return [np.mean(x, axis = 0) for x in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_, b_, g_, L, R, C = average_HbgLRC(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.real(np.linalg.eig(H_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    H_, b_, g_, L, J_R, J_C = average_HbgLRC(repetitions = 20)\n",
    "    print(L, J_R, J_C)\n",
    "    solve_CPO(H_, b_, g_, L, J_R, J_C, delta = 0.001, d = 20, H_lambda = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.Monitor(env, './video')\n",
    "get_rollout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Saver().save(sess, './cartpole-h1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Saver().restore(sess, './cartpole-h1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient with average gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    g_, L, R, C = average_gLRC(repetitions = 50)\n",
    "    print(L, R, C, C / R)\n",
    "    theta_k = np.array(list(iterate_flatten(sess.run(params))))\n",
    "    theta_new = theta_k + g_.flatten() * LR\n",
    "    load_params(params, theta_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking line search playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
